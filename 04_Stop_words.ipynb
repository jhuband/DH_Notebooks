{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Digital Humanities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit #4: Stop Words\n",
    "\n",
    "* Overview\n",
    "* Loading the NLTK Stop Words\n",
    "* Removing Stop Words from our Text\n",
    "* Creating our own List of Stop Words\n",
    "\n",
    "\n",
    "<font color=blue>---------------------------------------------------------------</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Overview \n",
    "\n",
    "\n",
    "Some words, like \"is\", \"the\", \"and\", occur frequently in text but have little semantic content. If we want to know the 25 most commonly-used words in a text, these words would top the list.  Unfortunately, they would not give us any insight into the topics that the text focuses on.  In text analysis, we call these _stop words_ and generally we would wnat to remove them before we do any analysis on the text.\n",
    "\n",
    "That, in turn, means that we would need to have a list of words that we deem to be unnecessary in our analysis.  While we could sit and come up with a list of our own stop words, it might be beneficial to start with a list of word that other programmers have already created.  There is such a list in the `nltk` package.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2  Loading the NLTK Stop Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list may not be complete, but it is a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3  Removing Stopwords from our Text\n",
    "\n",
    "Now that we have a list of words to remove, we need to learn how to remove them.  What you are about to see requires more advanced Python skills.  We will learn about these skills as we cover more about Python.  For now, we'll call it a magic formula.\n",
    "\n",
    " \n",
    "<font color=blue>---------------------------------------------------------------</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Emma', 'Jane', 'Austen', '1816', 'VOLUME', 'CHAPTER', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'rich', ',', 'comfortable', 'home', 'happy', 'disposition', ',', 'seemed', 'unite', 'best', 'blessings', 'existence', ';', 'lived', 'nearly', 'twenty-one', 'years', 'world', 'little', 'distress', 'vex', '.', 'youngest', 'two', 'daughters', 'affectionate', ',', 'indulgent', 'father', ';', ',', 'consequence', 'sister', \"'s\", 'marriage', ',', 'mistress', 'house', 'early', 'period', '.', 'mother', 'died', 'long', 'ago', 'indistinct', 'remembrance', 'caresses', ';', 'place', 'supplied', 'excellent', 'woman', 'governess', ',', 'fallen', 'little', 'short', 'mother', 'affection', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# Read the Emma excerpt\n",
    "with open(\"austen-emma-excerpt.txt\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "# Convert to a list of words\n",
    "words = nltk.word_tokenize(raw_text)\n",
    "\n",
    "# Perform the magic\n",
    "reduced_words = [w for w in words if w.lower() not in stop_words]\n",
    "print(reduced_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Extra Punctuation\n",
    "\n",
    "The words look much better; however, we are seeing punctuation listed as words.  We could handle this in a couple of ways.  First, we could remove punctuation from our words before we apply the stop words.  This would require another magical formula. Or, we could include punctuation in our list of stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>---------------------------------------------------------------</font>\n",
    "\n",
    "### 4.4 Creating your own List of Stop Words\n",
    "\n",
    "There is no such thing as a perfect list of stop words.  We may have to keep tweaking them until we have a comprehensive list for our corpa.  So, it is important to know how to extend the list of stop words that we extracted from `nltk`. \n",
    "\n",
    "Specifically, let's see how to include punctuation, like commas, periods, semicolons, and even apostrophe s.  \n",
    "\n",
    "To do this, we can an \"extend\" tool to merge our list with the existing list of stop words.  This can be done in two lines:\n",
    "```\n",
    "our_stopwords = [\",\", \".\", \";\", \"'s\"]\n",
    "stop_words.extend(our_stopwords)\n",
    "```\n",
    "\n",
    "Let's see it in action with the complete code.\n",
    "\n",
    "\n",
    "\n",
    "<font color=blue>---------------------------------------------------------------</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Emma', 'Jane', 'Austen', '1816', 'VOLUME', 'CHAPTER', 'Emma', 'Woodhouse', 'handsome', 'clever', 'rich', 'comfortable', 'home', 'happy', 'disposition', 'seemed', 'unite', 'best', 'blessings', 'existence', 'lived', 'nearly', 'twenty-one', 'years', 'world', 'little', 'distress', 'vex', 'youngest', 'two', 'daughters', 'affectionate', 'indulgent', 'father', 'consequence', 'sister', 'marriage', 'mistress', 'house', 'early', 'period', 'mother', 'died', 'long', 'ago', 'indistinct', 'remembrance', 'caresses', 'place', 'supplied', 'excellent', 'woman', 'governess', 'fallen', 'little', 'short', 'mother', 'affection']\n"
     ]
    }
   ],
   "source": [
    "# Load the stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# Extend the stop words with our own\n",
    "our_stopwords = [\",\", \".\", \";\", \"'s\"]\n",
    "stop_words.extend(our_stopwords)\n",
    "\n",
    "\n",
    "# Read the Emma excerpt\n",
    "with open(\"austen-emma-excerpt.txt\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "# Convert to a list of words\n",
    "words = nltk.word_tokenize(raw_text)\n",
    "\n",
    "# Perform the magic\n",
    "reduced_words = [w for w in words if w.lower() not in stop_words]\n",
    "print(reduced_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Activity:  Using Stop Words on a Larger File\n",
    "\n",
    "\n",
    "Make sure that you have downloaded the file \"emma_chapter_one.txt\".\n",
    "Read in the contents of that file and remove your stop words from the file.  Are there any more words that you feel should be added to the stop words?\n",
    "\n",
    "    \n",
    "<font color=blue>---------------------------------------------------------------</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this section, we learned how to remove commonly-used words from our text before we do any significant text analysis.  We also saw how we could create a more extensive list of stop words.  \n",
    "\n",
    "We will want to save the extended list of stop words for future use.  Before we can do that, we will need to learn a little more Python.\n",
    "\n",
    "\n",
    "<font color=blue>---------------------------------------------------------------</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
