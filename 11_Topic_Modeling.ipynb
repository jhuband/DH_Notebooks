{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit #11 Topic Modeling\n",
    "\n",
    "* Overview\n",
    "* LDA\n",
    "* Example Code\n",
    "\n",
    "<font color=blue>---------------------------------------------------------------</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Overview\n",
    "\n",
    "Topic Modeling is a machine learning technique for determining what topics are in a document or set of documents.  It is an _unsupervised_ technique in that it doesn't require pre-defined information to _learn_ how to identify topics.  Instead it has performs a series of statistical approaches to identify patterns of words.\n",
    "\n",
    "\n",
    "In this unit, we will look into using a _Latent Dirichlet Allocation_ (LDA) algorithm to search for topics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 LDA\n",
    "\n",
    "LDA assumes that documents are a collection of topics.  Topics will have words related to those topics mixed in with other words. LDA tries to work backwards, to determine the topics from the given the set of word.  It uses statistical analysis and pattern recognition to accomplish the task.\n",
    "\n",
    "To use LDA, we will \n",
    "- Read in one or more documents as text;\n",
    "- Pre-process the text (e.g., convert to lower case, remove stop words;\n",
    "- Create a dictionary of unique words and a **document-term matrix** -- I'll explain in a minute;\n",
    "- Feed into the LDA algorithm \n",
    "    - the document-term matrix;\n",
    "    - the dictionary;\n",
    "    - the number of topics we want it to find;\n",
    "    - the number of iterations we want the algorithm to perform to fine-tune the results\n",
    "- Display the resulting words for the topics\n",
    "    \n",
    "One of the preprocessing steps that we have not seen before is the converson of our bag of words to a **document-term matrix**. Basically, this is a list of counts for each unique word in the document.  But, instead of keeping the word with its count, we will create a pair of numbers where the first number is the position of the word in the dictionary and the second number is the count for that word.  For example, `(5, 12)` would mean that the word in position 5 (i.e., sixth word) of the dictionary occurs 12 times.  Note:  This dictionary is not the same as the dictionary data type that we learned about.  \n",
    "\n",
    "The other thing that you should note is that however many topics that you tell LDA to find, it will find that number.  If you over-estimate the number of topics, you may see significant overlap in the topic words.  Of course, this will depend on how large the body of work is.  Larger amounts will have more subtopics. \n",
    "\n",
    "Finally, because LDA returns a list of words for each of the topics, it will be up to us to interpret what the topic actually is.\n",
    "\n",
    "Now, we are ready to look at an example.  Before you attempt to run the example yourself, you will need to install `gensim`.  You can do this by typing `!pip install gensim` in iPython console.  It may take several minutes to install.\n",
    "\n",
    "<font color=blue>---------------------------------------------------------------</font>\n",
    "\n",
    "### 11.3 Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This example borrowed from \n",
    "# http://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_list = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for doc in doc_list:\n",
    "    doc_lower = doc.lower()\n",
    "    tokens = tokenizer.tokenize(doc_lower)\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [w for w in tokens if w not in stop_words]\n",
    "      \n",
    "\n",
    "    # stem token\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts.append(stemmed_tokens)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(32 unique tokens: ['brocolli', 'brother', 'eat', 'good', 'like']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)\n",
    "#The dictionary determines how many unique words that we have in all of the texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)], [(1, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(8, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(1, 1), (5, 1), (8, 1), (19, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)], [(0, 1), (3, 1), (16, 2), (30, 1), (31, 1)]]\n"
     ]
    }
   ],
   "source": [
    "#We now want to convert the texts to vectors based on the dictionary\n",
    "doc_term_matrix = [dictionary.doc2bow(text) for text in texts]\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# Generate an LDA model\n",
    "# The more passes, the better the results\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(doc_term_matrix, # the vector for the words\n",
    "                                           num_topics=5,    # how many topics we suspect\n",
    "                                           id2word = dictionary, # list of words for vector\n",
    "                                           passes=20)  # number of iterations we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.031*\"mother\" + 0.031*\"brother\" + 0.031*\"drive\" + 0.031*\"say\" + 0.031*\"profession\" + 0.031*\"pressur\" + 0.031*\"health\"')\n",
      "(1, '0.078*\"mother\" + 0.078*\"brother\" + 0.078*\"drive\" + 0.078*\"spend\" + 0.078*\"practic\" + 0.078*\"lot\" + 0.078*\"basebal\"')\n",
      "(2, '0.102*\"good\" + 0.102*\"brocolli\" + 0.102*\"health\" + 0.070*\"eat\" + 0.038*\"tension\" + 0.038*\"suggest\" + 0.038*\"caus\"')\n",
      "(3, '0.065*\"mother\" + 0.065*\"brother\" + 0.065*\"pressur\" + 0.065*\"drive\" + 0.065*\"perform\" + 0.065*\"well\" + 0.065*\"better\"')\n",
      "(4, '0.031*\"brother\" + 0.031*\"mother\" + 0.031*\"drive\" + 0.031*\"profession\" + 0.031*\"say\" + 0.031*\"pressur\" + 0.031*\"health\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_topics=5, num_words=7)\n",
    "for item in topics:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Topic Modeling on Emma Chapter 1\n",
    "\n",
    "Try applying LDA to \"emma_chapter_one\".  Because this is a single document, you will include just it in the `doc_list`.  For example, \n",
    "```\n",
    "doc_list = [raw_text]\n",
    "```\n",
    "\n",
    "\n",
    "<font color=blue>---------------------------------------------------------------</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
